# -*- coding: utf-8 -*-
"""Data_Exploration.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G6N27md5UEcBI8gQlait7QWux5k3DDNm

#Bank Customer Churn Prediction
"""





"""#Data Exploration part

This module consists of data pre-processing for the project.

Raw Data for project is already collected (Source: Kaggle)
"""

#import libraries
import numpy as np

import pandas as pd

import matplotlib.pyplot as plt

import seaborn as sns

import plotly.express as px

import plotly.graph_objects as go

from matplotlib.ticker import PercentFormatter

from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier

from sklearn.model_selection import train_test_split,GridSearchCV

from sklearn.neural_network import MLPClassifier

from sklearn.pipeline import Pipeline

from sklearn.linear_model import LogisticRegression

from sklearn.preprocessing import StandardScaler

from sklearn.tree import DecisionTreeClassifier

from sklearn import svm

from sklearn.preprocessing import LabelEncoder

from sklearn import preprocessing

from sklearn.metrics import roc_curve, roc_auc_score,accuracy_score,make_scorer

from sklearn.metrics import recall_score,classification_report,precision_score,confusion_matrix

# loading dataset to IDE as DataFrame

data = pd.read_csv('/content/(Raw_Data)Bank Customer Churn Prediction.csv')

data.head()

#Information about dataset

data.info()

#removing unnecessary attributes

data = data.drop('customer_id',axis=1)

#Checking for duplicated values

data.duplicated().sum()

#checking for null(empty entry) values

data.isnull().sum()

data['balance']

#Replacing null values with average(mean) value of the attribute



data['balance'].fillna(data['balance'].mean(), inplace=True)

data

# changing binary variables of attributes 'credit_card','active_member' and 'churn' to 0 & 1 for easy mathematical purpose

data['credit_card'] = data['credit_card'].replace(to_replace = ['YES','NO'],value = ['1','0'])

data['active_member'] = data['active_member'].replace(to_replace = ['ACTIVE','NON_ACTIVE'],value = ['1','0'])

data['churn'] = data['churn'].replace(to_replace = ['YES','NO'],value = ['1','0'])

data

data.head(10)

data.tail(10)

#Downloading pre-processed dataframe as csv

data.to_csv('Data_exploration.csv', header=True, index=False)

files.download('Data_exploration.csv')

"""#Feature Engineering


In this portion we apply feature engineering concept on our explored and cleaned dataset.

In thi we develop new attribute using existing attributes
"""

#loading csv file into dataframe using pandads

feature_data = pd.read_csv('/content/(Data_exploration)Bank Customer Churn Prediction.csv')

feature_data

# We use two attributes 'credit_card' & 'churn' to develop a new feature named 'cc_with_churn'.

feature_data['cc_with_churn'] = np.where((feature_data['credit_card'] == 1) & (feature_data['churn'] == 1),1,0)

feature_data

# We use two attributes 'active_member' & 'churn' to develop a new feature named 'am_with_churn'.

feature_data['am_with_churn'] = np.where((feature_data['active_member'] == 1) & (feature_data['churn'] == 1),1,0)

feature_data

feature_data

#Downloading Feature Engineering dataframe as csv

feature_data.to_csv('(Feature Engineering) Bank Customer Churn Prediction.csv', header=True, index=False)

files.download('(Feature Engineering) Bank Customer Churn Prediction.csv')

"""#Data Analysis & Visualization

In this portion we do some exploratory analysis on dataset for finding the patterns.

By using visualizations we try to find patterns.
"""

# loading feature engineering applied dataset

featured_data = pd.read_csv('/content/(Feature Engineering) Bank Customer Churn Prediction.csv')

featured_data

# creating dummy dataset for analysis

dummy = featured_data

dummy.head()

"""#### Churn Distibution on different Perspectives"""

# Anlayzing churn distribution

plt.bar(['Churn', 'Retention'], [dummy['churn'].sum(), dummy['churn'].shape[0]-dummy['churn'].sum().sum()])

plt.title('Churn distribution')

print("Churn:",dummy['churn'].sum())

print("Retention:",dummy['churn'].shape[0]-dummy['churn'].sum().sum())

# Anlayzing churn distribution for credit_card holders

plt.bar(['Churn', 'Retention'], [dummy['cc_with_churn'].sum(), dummy['cc_with_churn'].shape[0]-dummy['cc_with_churn'].sum().sum()])

plt.title('Churn distribution in credit card holders')

print("Churn:",dummy['cc_with_churn'].sum())

print("Retention:",dummy['cc_with_churn'].shape[0]-dummy['cc_with_churn'].sum().sum())

# Anlayzing churn distribution for Active Members

plt.bar(['Churn', 'Retention'], [dummy['am_with_churn'].sum(), dummy['am_with_churn'].shape[0]-dummy['am_with_churn'].sum().sum()])

plt.title('Churn distribution in active members')

print("Churn:",dummy['am_with_churn'].sum())

print("Retention:",dummy['am_with_churn'].shape[0]-dummy['am_with_churn'].sum().sum())

# Creating new attribute for those who are cc holders and active members and churn

dummy['cc_am_with_churn'] = np.where((dummy['cc_with_churn'] == 1) & (dummy['am_with_churn'] == 1), 1, 0)

# Anlayzing churn distribution for cc holders & Active Members

plt.bar(['Churn', 'Retention'], [dummy['cc_am_with_churn'].sum(), dummy['cc_am_with_churn'].shape[0]-dummy['cc_am_with_churn'].sum().sum()])

plt.title('Churn distribution who are cc holders and active members')

print("Churn:",dummy['cc_am_with_churn'].sum())

print("Retention:",dummy['cc_am_with_churn'].shape[0]-dummy['cc_am_with_churn'].sum().sum())

"""#### Male-Female Distribution on different perspectives"""

# Making gender attribute as integer

dummy['gender'] = dummy['gender'].replace(to_replace = ['Male','Female'],value = ['1','0']).astype(int)

# Male - Female Distribution

plt.bar(['Male', 'Female'], [dummy['gender'].sum(), dummy['gender'].shape[0]-dummy['gender'].sum().sum()])

plt.title('Male -Female ratio on churn distribution')

print("Male:",dummy['gender'].sum())

print("Female:",dummy['gender'].shape[0]-dummy['gender'].sum().sum())

# Male - Female distribution who churn

# Creating dataframe which only shows who are churn
dummy1 = dummy.loc[dummy['churn'] == 1]

# Visualizing Male - Female Distibution who churn
plt.bar(['Male', 'Female'], [dummy1['gender'].sum(), dummy1['gender'].shape[0]-dummy1['gender'].sum().sum()])

plt.title('Male -Female ratio on churn distribution')

print("Male:",dummy1['gender'].sum())

print("Female:",dummy1['gender'].shape[0]-dummy1['gender'].sum().sum())

# Male - Female distribution who churn and cc holders

# Creating dataframe which only shows who are churn
dummy2 = dummy.loc[dummy['cc_with_churn'] == 1]

# Visualizing Male - Female Distibution who churn
plt.bar(['Male', 'Female'], [dummy2['gender'].sum(), dummy2['gender'].shape[0]-dummy2['gender'].sum().sum()])

plt.title('Male -Female ratio on churn distribution who are cc holders')

print("Male:",dummy2['gender'].sum())

print("Female:",dummy2['gender'].shape[0]-dummy2['gender'].sum().sum())

# Male - Female distribution who churn and Active Members

# Creating dataframe which only shows who are churn
dummy3 = dummy.loc[dummy['am_with_churn'] == 1]

# Visualizing Male - Female Distibution who churn
plt.bar(['Male', 'Female'], [dummy3['gender'].sum(), dummy3['gender'].shape[0]-dummy3['gender'].sum().sum()])

plt.title('Male -Female ratio on churn distribution who are Active members')

print("Male:",dummy3['gender'].sum())

print("Female:",dummy3['gender'].shape[0]-dummy3['gender'].sum().sum())

# Male - Female distribution who churn and (Active Members & cc holders)

# Creating dataframe which only shows who are churn
dummy4 = dummy.loc[dummy['cc_am_with_churn'] == 1]

# Visualizing Male - Female Distibution who churn
plt.bar(['Male', 'Female'], [dummy4['gender'].sum(), dummy4['gender'].shape[0]-dummy4['gender'].sum().sum()])

plt.title('Male -Female ratio on churn distribution who are cc holders and AM')

print("Male:",dummy4['gender'].sum())

print("Female:",dummy4['gender'].shape[0]-dummy4['gender'].sum().sum())

"""##### Findings : Female customers are more likely to churn that's why banks have to work on more towards women engagement programs

#### Churn distribution in different Countries
"""

# creating pie chart for customer distribution in different Countries

pd.crosstab(index=dummy.country,columns='count',normalize=True).plot(
    kind='pie',autopct='%.1f%%',y='count',
    labels=dummy.country.unique(),
    ylabel='',title='Countries',legend=False)

# Calculating churn ratio for each country

by_country = dummy.groupby(['country'])['churn'].value_counts(normalize=True).to_frame().rename(columns={'churn': 'ratio'}).reset_index().sort_values('country')

by_country

# Visualizing churn ratio for each country

sns.barplot(x='country',y='ratio', hue='churn', data=by_country).set(title = 'Churned Probability with country')

# Calculating churn ratio for who are cc holders each country

by_country_cc = dummy.groupby(['country'])['cc_with_churn'].value_counts(normalize=True).to_frame().rename(columns={'cc_with_churn': 'ratio'}).reset_index().sort_values('country')

by_country_cc

# Visualizing churn ratio for who are cc holder each country

sns.barplot(x='country',y='ratio', hue='cc_with_churn', data=by_country_cc).set(title = 'Churned Probability with country aho are cc holder')

# Calculating churn ratio for who are Active members each country

by_country_am = dummy.groupby(['country'])['am_with_churn'].value_counts(normalize=True).to_frame().rename(columns={'am_with_churn': 'ratio'}).reset_index().sort_values('country')

by_country_am

# Visualizing churn ratio for who are Active Members each country

sns.barplot(x='country',y='ratio', hue='am_with_churn', data=by_country_am).set(title = 'Churned Probability with country aho are active Members')

# Calculating churn ratio for who are Active members & cc holders each country

by_country_cc_am = dummy.groupby(['country'])['cc_am_with_churn'].value_counts(normalize=True).to_frame().rename(columns={'cc_am_with_churn': 'ratio'}).reset_index().sort_values('country')

by_country_cc_am

# Visualizing churn ratio for who are Active Members each country

sns.barplot(x='country',y='ratio', hue='cc_am_with_churn', data=by_country_cc_am).set(title = 'Churned Probability with country aho are active Members & cc holders')

"""##### Findings : Germany has highest churn rate

In germany churn rate for cc holders,active members and both is respectively 0.23,0.12,0.08

Whereas France and Spain has half of churn rate than Germany

#### By gender and country wise churn ratio on differnt perspectives
"""

# Calculating churn ratio on gender & country wise

by_gender_country = dummy.groupby(['country', 'gender'])['churn'].value_counts(normalize=True).to_frame().rename(columns={'churn': 'ratio'}).reset_index().sort_values('country')

by_gender_country

# Visualizing churn ratio on gender & country wise

sns.catplot(x='gender',y='ratio', hue='churn', col='country', kind='bar', data=by_gender_country)

# Calculating churn ratio on gender & country wise who are cc holders

by_gender_country_cc = dummy.groupby(['country', 'gender'])['cc_with_churn'].value_counts(normalize=True).to_frame().rename(columns={'cc_with_churn': 'ratio'}).reset_index().sort_values('country')

by_gender_country_cc

# Visualizing churn ratio on gender & country wise who are cc holders

sns.catplot(x='gender',y='ratio', hue='cc_with_churn', col='country', kind='bar', data=by_gender_country_cc)

# Calculating churn ratio on gender & country wise who are Active Members
by_gender_country_am = dummy.groupby(['country', 'gender'])['am_with_churn'].value_counts(normalize=True).to_frame().rename(columns={'am_with_churn': 'ratio'}).reset_index().sort_values('country')

by_gender_country_am

# Visualizing churn ratio on gender & country wise who are Active Members

sns.catplot(x='gender',y='ratio', hue='am_with_churn', col='country', kind='bar', data=by_gender_country_am)

# Calculating churn ratio on gender & country wise who are Active Members & cc holders

by_gender_country_cc_am = dummy.groupby(['country', 'gender'])['cc_am_with_churn'].value_counts(normalize=True).to_frame().rename(columns={'cc_am_with_churn': 'ratio'}).reset_index().sort_values('country')

by_gender_country_cc_am

# Visualizing churn ratio on gender & country wise who are Active Members & cc holders

sns.catplot(x='gender',y='ratio', hue='cc_am_with_churn', col='country', kind='bar', data=by_gender_country_cc_am)

"""#### Using age for churn distribution"""

# Calculating churn ratio with age

by_age = dummy.groupby(['age'])['churn'].value_counts(normalize=True).to_frame().rename(columns={'churn': 'ratio'}).reset_index().sort_values('age')

by_age

# Visualizing churn ratio with age

sns.catplot(x='age',y='ratio', hue='churn',  kind='bar', data=by_age,height=10,aspect=2).set(title = 'Churned Probability with age')

# Calculating churn ratio with age who are cc holders

by_age_cc = dummy.groupby(['age'])['cc_with_churn'].value_counts(normalize=True).to_frame().rename(columns={'cc_with_churn': 'ratio'}).reset_index().sort_values('age')

by_age_cc

# Visualizing churn ratio with age who are cc holders

sns.catplot(x='age',y='ratio', hue='cc_with_churn',  kind='bar', data=by_age_cc,height=10,aspect=2).set(title = 'Churned Probability with age')

"""#####Findings :


Most people between the ages of 49 and 57 would like to churn.

#### Using balance for churn prediction
"""

# balance vs churn

sns.displot(dummy[(dummy['churn'] == 1)]['balance'], label='Churned').set(xlim=(0))
plt.legend(loc= 'upper right')
sns.displot(dummy[(dummy['churn'] == 0)]['balance'], label='Not Churned').set(xlim=(0))
plt.legend(loc= 'upper right')
plt.show()

"""#### Tenure vs churn"""

# Tenure vs churn

sns.displot(dummy[dummy['churn'] == 1]['tenure'], label='Churned', kde=True).set(xlim=(0))
plt.legend(loc= 'upper right')
sns.displot(dummy[dummy['churn'] == 0]['tenure'], label='Not Churned', kde=True).set(xlim=(0))
plt.legend(loc= 'upper right')
plt.show()

"""#### Product number vs churn"""

#Visualizing product number vs churn

by_products_number = dummy.groupby(['products_number','country'])['churn'].value_counts(normalize=True).to_frame().rename(columns={'churn': 'ratio'}).reset_index().sort_values('products_number')

print(by_products_number)

sns.catplot(x='products_number',y='ratio', hue='churn', kind='bar', data=by_products_number,height=4,aspect=2).set(title = 'Churned Probability with products_number')

#Visualizing product number vs churn who are cc holders

by_products_number_cc = dummy.groupby(['products_number','country'])['cc_with_churn'].value_counts(normalize=True).to_frame().rename(columns={'cc_with_churn': 'ratio'}).reset_index().sort_values('cc_with_churn')

print(by_products_number_cc)

sns.catplot(x='products_number',y='ratio', hue='cc_with_churn', kind='bar', data=by_products_number_cc,height=4,aspect=2).set(title = 'Churned Probability with products_number')

dummy[dummy['products_number']==4].products_number.count()

"""#####Findings :

The probability of churn becomes greater as the product number grows, which means that customers with multiple services will feel uncomfortable

Even 60 people with a product number of 4 have to churn

In spain who has credit card and product number 4 has 100% churn

# Machine Learning Model Building
"""

# Splitting data into train and test

features = list(featured_data.columns)

target = "churn"

features.remove(target)


X = featured_data[features]

y = featured_data[target]


X = X.apply(pd.to_numeric, errors='coerce')

y = y.apply(pd.to_numeric, errors='coerce')


X.fillna(0, inplace = True)

y.fillna(0, inplace = True)


X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.3) #70% part of data is considered for training dataset and 30% part of data is considered for testing dataset


print(X_train.shape)

print(X_test.shape)

"""### Logistic Regression"""

# building a logistic regression model

steps = [('rescale', StandardScaler()),
         ('logr', LogisticRegression())]

modelLr = Pipeline(steps)

modelLr = modelLr.fit(X_train, y_train)

y_train_pred = modelLr.predict(X_train)

LRpred = modelLr.predict(X_test)

print(classification_report(y_test,LRpred))

cf_matrixLr = confusion_matrix(y_test,LRpred)


group_names = ['True Neg','False Pos','False Neg','True Pos']

group_counts = ["{0:0.0f}".format(value) for value in
                cf_matrixLr.flatten()]

group_percentages = ['{0:.2%}'.format(value) for value in
                     cf_matrixLr.flatten()/np.sum(cf_matrixLr)]

labels = [f'{v1}\n{v2}\n{v3}' for v1, v2, v3 in
          zip(group_names,group_counts,group_percentages)]

labels = np.asarray(labels).reshape(2,2)

sns.heatmap(cf_matrixLr, annot = labels, fmt = '', cmap = 'Blues')

all_sample_title = 'Accuracy Score: {0}'.format(modelLr.score(X_test, y_test))

plt.title(all_sample_title, size = 15)

"""### Decision Tree"""

steps = [('rescale', StandardScaler()),
         ('dt', DecisionTreeClassifier())]

dtree = Pipeline(steps)

dtree.fit(X_train, y_train)


y_train_pred = dtree.predict(X_train)

DTpred = dtree.predict(X_test)

print(classification_report(y_test,DTpred))

cf_matrixLr = confusion_matrix(y_test,DTpred)

group_names = ['True Neg','False Pos','False Neg','True Pos']

group_counts = ["{0:0.0f}".format(value) for value in
                cf_matrixLr.flatten()]

group_percentages = ['{0:.2%}'.format(value) for value in
                     cf_matrixLr.flatten()/np.sum(cf_matrixLr)]

labels = [f'{v1}\n{v2}\n{v3}' for v1, v2, v3 in
          zip(group_names,group_counts,group_percentages)]

labels = np.asarray(labels).reshape(2,2)

sns.heatmap(cf_matrixLr, annot = labels, fmt = '', cmap = 'Blues')

all_sample_title = 'Accuracy Score: {0}'.format(dtree.score(X_test, y_test))

plt.title(all_sample_title, size = 15)

"""### Support Vector Machine (SVM)"""

#Building SVM for our dataset

steps = [('rescale', StandardScaler()),
         ('svm', DecisionTreeClassifier())]

svm = Pipeline(steps)

svm.fit(X_train, y_train)


y_train_pred = svm.predict(X_train)

SVMpred = svm.predict(X_test)

print(classification_report(y_test,SVMpred))

cf_matrixLr = confusion_matrix(y_test,SVMpred)

group_names = ['True Neg','False Pos','False Neg','True Pos']

group_counts = ["{0:0.0f}".format(value) for value in
                cf_matrixLr.flatten()]

group_percentages = ['{0:.2%}'.format(value) for value in
                     cf_matrixLr.flatten()/np.sum(cf_matrixLr)]

labels = [f'{v1}\n{v2}\n{v3}' for v1, v2, v3 in
          zip(group_names,group_counts,group_percentages)]

labels = np.asarray(labels).reshape(2,2)

sns.heatmap(cf_matrixLr, annot = labels, fmt = '', cmap = 'Blues')

all_sample_title = 'Accuracy Score: {0}'.format(svm.score(X_test, y_test))

plt.title(all_sample_title, size = 15)